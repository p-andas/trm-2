{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "33557dc0-9081-4088-ae08-fc330a5a29ef",
   "metadata": {},
   "source": [
    "# inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "875657ca-5bc1-4fc2-8f76-d87967f786fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "SUDOKU-EXTREME TEST SET EVALUATION\n",
      "================================================================================\n",
      "\n",
      "1. Loading configuration...\n",
      "   ✓ Config loaded\n",
      "\n",
      "2. Loading test data...\n",
      "   ✓ Test examples: 422,786\n",
      "\n",
      "3. Loading model...\n",
      "   ✓ Model loaded (5.03M parameters)\n",
      "\n",
      "4. Running inference on 422,786 test puzzles...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:   6%|▌         | 96/1652 [02:54<47:00,  1.81s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 117\u001b[0m\n\u001b[1;32m    114\u001b[0m max_steps \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m16\u001b[39m \n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(max_steps):\n\u001b[0;32m--> 117\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcarry\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcarry\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    119\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(result) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[1;32m    120\u001b[0m         carry, outputs \u001b[38;5;241m=\u001b[39m result\n",
      "File \u001b[0;32m/opt/micromamba/envs/python_310/lib/python3.10/site-packages/torch/nn/modules/module.py:1775\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1774\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1775\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/micromamba/envs/python_310/lib/python3.10/site-packages/torch/nn/modules/module.py:1786\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1782\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1783\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1784\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1785\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1788\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1789\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/workspace/TinyRecursiveModels/models/recursive_reasoning/trm.py:259\u001b[0m, in \u001b[0;36mTinyRecursiveReasoningModel_ACTV1.forward\u001b[0;34m(self, carry, batch)\u001b[0m\n\u001b[1;32m    256\u001b[0m new_current_data \u001b[38;5;241m=\u001b[39m {k: torch\u001b[38;5;241m.\u001b[39mwhere(carry\u001b[38;5;241m.\u001b[39mhalted\u001b[38;5;241m.\u001b[39mview((\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, ) \u001b[38;5;241m+\u001b[39m (\u001b[38;5;241m1\u001b[39m, ) \u001b[38;5;241m*\u001b[39m (batch[k]\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m)), batch[k], v) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m carry\u001b[38;5;241m.\u001b[39mcurrent_data\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[1;32m    258\u001b[0m \u001b[38;5;66;03m# Forward inner model\u001b[39;00m\n\u001b[0;32m--> 259\u001b[0m new_inner_carry, logits, (q_halt_logits, q_continue_logits) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minner\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew_inner_carry\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnew_current_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    261\u001b[0m outputs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    262\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogits\u001b[39m\u001b[38;5;124m\"\u001b[39m: logits,\n\u001b[1;32m    263\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mq_halt_logits\u001b[39m\u001b[38;5;124m\"\u001b[39m: q_halt_logits,\n\u001b[1;32m    264\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mq_continue_logits\u001b[39m\u001b[38;5;124m\"\u001b[39m: q_continue_logits\n\u001b[1;32m    265\u001b[0m }\n\u001b[1;32m    267\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m    268\u001b[0m     \u001b[38;5;66;03m# Step\u001b[39;00m\n",
      "File \u001b[0;32m/opt/micromamba/envs/python_310/lib/python3.10/site-packages/torch/nn/modules/module.py:1775\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1774\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1775\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/micromamba/envs/python_310/lib/python3.10/site-packages/torch/nn/modules/module.py:1786\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1782\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1783\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1784\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1785\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1788\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1789\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/workspace/TinyRecursiveModels/models/recursive_reasoning/trm.py:211\u001b[0m, in \u001b[0;36mTinyRecursiveReasoningModel_ACTV1_Inner.forward\u001b[0;34m(self, carry, batch)\u001b[0m\n\u001b[1;32m    209\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m _H_step \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mH_cycles\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m    210\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m _L_step \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mL_cycles):\n\u001b[0;32m--> 211\u001b[0m             z_L \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mL_level\u001b[49m\u001b[43m(\u001b[49m\u001b[43mz_L\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mz_H\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43minput_embeddings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mseq_info\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    212\u001b[0m         z_H \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mL_level(z_H, z_L, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mseq_info)\n\u001b[1;32m    213\u001b[0m \u001b[38;5;66;03m# 1 with grad\u001b[39;00m\n",
      "File \u001b[0;32m/opt/micromamba/envs/python_310/lib/python3.10/site-packages/torch/nn/modules/module.py:1775\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1774\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1775\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/micromamba/envs/python_310/lib/python3.10/site-packages/torch/nn/modules/module.py:1786\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1782\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1783\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1784\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1785\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1788\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1789\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/workspace/TinyRecursiveModels/models/recursive_reasoning/trm.py:114\u001b[0m, in \u001b[0;36mTinyRecursiveReasoningModel_ACTV1ReasoningModule.forward\u001b[0;34m(self, hidden_states, input_injection, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m hidden_states \u001b[38;5;241m+\u001b[39m input_injection\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[0;32m--> 114\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m hidden_states\n",
      "File \u001b[0;32m/opt/micromamba/envs/python_310/lib/python3.10/site-packages/torch/nn/modules/module.py:1775\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1774\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1775\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/micromamba/envs/python_310/lib/python3.10/site-packages/torch/nn/modules/module.py:1786\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1782\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1783\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1784\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1785\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1788\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1789\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/workspace/TinyRecursiveModels/models/recursive_reasoning/trm.py:96\u001b[0m, in \u001b[0;36mTinyRecursiveReasoningModel_ACTV1Block.forward\u001b[0;34m(self, cos_sin, hidden_states)\u001b[0m\n\u001b[1;32m     94\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m hidden_states\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m     95\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmlp_t(hidden_states)\n\u001b[0;32m---> 96\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[43mrms_norm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvariance_epsilon\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm_eps\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     97\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m hidden_states\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m     98\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     99\u001b[0m     \u001b[38;5;66;03m# Self Attention\u001b[39;00m\n",
      "File \u001b[0;32m/workspace/TinyRecursiveModels/models/layers.py:167\u001b[0m, in \u001b[0;36mrms_norm\u001b[0;34m(hidden_states, variance_epsilon)\u001b[0m\n\u001b[1;32m    164\u001b[0m input_dtype \u001b[38;5;241m=\u001b[39m hidden_states\u001b[38;5;241m.\u001b[39mdtype\n\u001b[1;32m    165\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m hidden_states\u001b[38;5;241m.\u001b[39mto(torch\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[0;32m--> 167\u001b[0m variance \u001b[38;5;241m=\u001b[39m \u001b[43mhidden_states\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msquare\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmean\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeepdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    168\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m hidden_states \u001b[38;5;241m*\u001b[39m torch\u001b[38;5;241m.\u001b[39mrsqrt(variance \u001b[38;5;241m+\u001b[39m variance_epsilon)\n\u001b[1;32m    169\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m hidden_states\u001b[38;5;241m.\u001b[39mto(input_dtype)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.insert(0, \"/workspace/TinyRecursiveModels\")\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import yaml\n",
    "\n",
    "from models.recursive_reasoning.trm import TinyRecursiveReasoningModel_ACTV1\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"SUDOKU-EXTREME TEST SET EVALUATION\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Paths\n",
    "CHECKPOINT_PATH = \"/workspace/TinyRecursiveModels/checkpoints/Sudoku-extreme-1k-aug-1000-ACT-torch/sudoku_mlp_h100_test/step_78120\"\n",
    "CONFIG_PATH = \"/workspace/TinyRecursiveModels/checkpoints/Sudoku-extreme-1k-aug-1000-ACT-torch/sudoku_mlp_h100_test/all_config.yaml\"\n",
    "TEST_DATA_DIR = \"/workspace/TinyRecursiveModels/data/sudoku-extreme-1k-aug-1000/test\"\n",
    "\n",
    "print(\"\\n1. Loading configuration...\")\n",
    "with open(CONFIG_PATH) as f:\n",
    "    config = yaml.safe_load(f)\n",
    "print(\"   ✓ Config loaded\")\n",
    "\n",
    "print(\"\\n2. Loading test data...\")\n",
    "test_inputs = np.load(f\"{TEST_DATA_DIR}/all__inputs.npy\")\n",
    "test_labels = np.load(f\"{TEST_DATA_DIR}/all__labels.npy\")\n",
    "print(f\"   ✓ Test examples: {len(test_inputs):,}\")\n",
    "\n",
    "print(\"\\n3. Loading model...\")\n",
    "checkpoint = torch.load(CHECKPOINT_PATH, map_location='cpu')\n",
    "\n",
    "# Detect vocab_size and num_puzzle_identifiers from checkpoint\n",
    "embed_key = [k for k in checkpoint.keys() if 'embed_tokens.embedding_weight' in k][0]\n",
    "puzzle_emb_key = [k for k in checkpoint.keys() if 'puzzle_emb.weights' in k][0]\n",
    "vocab_size = checkpoint[embed_key].shape[0]\n",
    "num_puzzle_identifiers = checkpoint[puzzle_emb_key].shape[0]\n",
    "\n",
    "# Model config\n",
    "model_config = {\n",
    "    'batch_size': config['global_batch_size'],\n",
    "    'seq_len': 81,\n",
    "    'num_puzzle_identifiers': num_puzzle_identifiers,\n",
    "    'vocab_size': vocab_size,\n",
    "    'hidden_size': config['arch']['hidden_size'],\n",
    "    'H_cycles': config['arch']['H_cycles'],\n",
    "    'L_cycles': config['arch']['L_cycles'],\n",
    "    'H_layers': config['arch']['H_layers'],\n",
    "    'L_layers': config['arch']['L_layers'],\n",
    "    'mlp_t': config['arch']['mlp_t'],\n",
    "    'pos_encodings': config['arch']['pos_encodings'],\n",
    "    'halt_max_steps': config['arch']['halt_max_steps'],\n",
    "    'expansion': config['arch']['expansion'],\n",
    "    'num_heads': config['arch']['num_heads'],\n",
    "    'forward_dtype': config['arch']['forward_dtype'],\n",
    "    'halt_exploration_prob': config['arch']['halt_exploration_prob'],\n",
    "    'puzzle_emb_len': config['arch']['puzzle_emb_len'],\n",
    "    'puzzle_emb_ndim': config['arch']['puzzle_emb_ndim'],\n",
    "    'no_ACT_continue': config['arch'].get('no_ACT_continue', True),\n",
    "}\n",
    "\n",
    "model = TinyRecursiveReasoningModel_ACTV1(model_config)\n",
    "\n",
    "# Load checkpoint weights\n",
    "clean_state_dict = {}\n",
    "for k, v in checkpoint.items():\n",
    "    new_k = k.replace('_orig_mod.model.', '').replace('module.', '')\n",
    "    clean_state_dict[new_k] = v\n",
    "\n",
    "model.load_state_dict(clean_state_dict, strict=False)\n",
    "\n",
    "# Move to GPU\n",
    "device = torch.device('cuda')\n",
    "model = model.to(device)\n",
    "if hasattr(model.inner, 'H_init'):\n",
    "    model.inner.H_init = model.inner.H_init.to(device)\n",
    "if hasattr(model.inner, 'L_init'):\n",
    "    model.inner.L_init = model.inner.L_init.to(device)\n",
    "\n",
    "model.eval()\n",
    "print(f\"   ✓ Model loaded ({sum(p.numel() for p in model.parameters()) / 1e6:.2f}M parameters)\")\n",
    "\n",
    "print(f\"\\n4. Running inference on {len(test_inputs):,} test puzzles...\")\n",
    "\n",
    "# Evaluation loop\n",
    "batch_size = 256\n",
    "num_correct = 0\n",
    "num_total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i in tqdm(range(0, len(test_inputs), batch_size), desc=\"Evaluating\"):\n",
    "        batch_inputs = torch.from_numpy(test_inputs[i:i+batch_size]).long().cuda()\n",
    "        batch_labels = torch.from_numpy(test_labels[i:i+batch_size]).long().cuda()\n",
    "        \n",
    "        batch = {\n",
    "            'inputs': batch_inputs,\n",
    "            'labels': batch_labels,\n",
    "            'puzzle_identifiers': torch.zeros(len(batch_inputs), dtype=torch.long, device='cuda'),\n",
    "        }\n",
    "        \n",
    "        # Initialize carry\n",
    "        with torch.cuda.device('cuda:0'):\n",
    "            carry = model.initial_carry(batch)\n",
    "            if hasattr(carry, 'inner_carry'):\n",
    "                if hasattr(carry.inner_carry, 'z_H') and carry.inner_carry.z_H.device.type != 'cuda':\n",
    "                    carry.inner_carry.z_H = carry.inner_carry.z_H.cuda()\n",
    "                if hasattr(carry.inner_carry, 'z_L') and carry.inner_carry.z_L.device.type != 'cuda':\n",
    "                    carry.inner_carry.z_L = carry.inner_carry.z_L.cuda()\n",
    "            if hasattr(carry, 'steps') and carry.steps.device.type != 'cuda':\n",
    "                carry.steps = carry.steps.cuda()\n",
    "            if hasattr(carry, 'halted') and carry.halted.device.type != 'cuda':\n",
    "                carry.halted = carry.halted.cuda()\n",
    "        \n",
    "        max_steps = 16 \n",
    "        \n",
    "        for step in range(max_steps):\n",
    "            result = model(carry=carry, batch=batch)\n",
    "            \n",
    "            if len(result) == 2:\n",
    "                carry, outputs = result\n",
    "            elif len(result) == 5:\n",
    "                carry, loss, metrics, outputs, all_finish = result\n",
    "            else:\n",
    "                raise ValueError(f\"Unexpected return format: {len(result)} values\")\n",
    "            \n",
    "        \n",
    "        # Get predictions\n",
    "        if 'logits' in outputs:\n",
    "            preds = torch.argmax(outputs['logits'], dim=-1)\n",
    "        elif 'preds' in outputs:\n",
    "            preds = outputs['preds']\n",
    "        else:\n",
    "            raise ValueError(f\"Cannot find predictions. Available: {outputs.keys()}\")\n",
    "        \n",
    "        # Exact accuracy: all 81 cells correct\n",
    "        exact_matches = (preds == batch_labels).all(dim=1)\n",
    "        num_correct += exact_matches.sum().item()\n",
    "        num_total += len(batch_inputs)\n",
    "\n",
    "# Results\n",
    "test_accuracy = 100.0 * num_correct / num_total\n",
    "\n",
    "print(f\"\\n\" + \"=\" * 80)\n",
    "print(\"RESULTS\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Test examples: {num_total:,}\")\n",
    "print(f\"Correctly solved: {num_correct:,}\")\n",
    "print(f\"Test accuracy: {test_accuracy:.2f}%\")\n",
    "print(f\"\\nPaper baseline (TRM-MLP): 87.4%\")\n",
    "print(f\"Our model: {test_accuracy:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c47bbf1-0e04-4382-8385-87c5d2f921ef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
